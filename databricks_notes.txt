
# Download a single file from s3 - by default file is stored to location "/dbfs/"
dbutils.fs.cp('s3://BUCKET/temp.csv', 'temp.csv')  # Downloads to '/dbfs/temp.csv'

# IAM Integration - the client can auto pick up the IAM role for use with boto3
import boto3
client = boto3.client('s3')
print("Role: ", boto3.client('sts').get_caller_identity()['Arn'])
print('\n'.join([item['Key'] for item in client.list_objects_v2(Bucket='s3-datascience-na')['Contents']]))

# Write to S3
(
    test_df
    .write
    .format("delta")
    .save(s3_table_location)
)

# Read from S3
display(
    test_df
    .read
    .format('delta')
    .mode('overwrite')
    .load(s3_table_location)
)
# Query Redshift
jdbc_url = "jdbc:redshift://{redshift_host}:{redshift_port}/{redshift_db};UID={redshift_user};PWD={redshift_password}".format(
    redshift_host=dbutils.secrets.get(scope=secret_scope, key="redshift_host"),
    redshift_db=dbutils.secrets.get(scope=secret_scope, key="redshift_db"),
    redshift_port=dbutils.secrets.get(scope=secret_scope, key="redshift_port"),
    redshift_user=dbutils.secrets.get(scope=secret_scope, key="redshift_user"),
    redshift_password=dbutils.secrets.get(scope=secret_scope, key="redshift_password"),
)

display(
    spark.read
    .format("com.databricks.spark.redshift")
    .option("forward_spark_s3_credentials", True)
    .option("url", jdbc_url)
    .option("query", "select top 10 * from postgres.companies")
    .option("tempdir", s3_temp_dir)
    .load()
)

# Tracking cost?

There is a CSV that can be automatically uploaded to your S3 bucket daily by doing some API calls.
This CSV contains an itemized table of job runs/cluster instances and their machine hours/DBUs.

The final thing you can do is map the machineHours field from that CSV by joining the instanceType field to the appropriate hourly cost of the EC2 instances.
The only difficulty is that you need to account for EBS volume size but this is probably uniform across your databricks clusters. This allows getting the
actual prices of the ec2 instances that AWS charges. Use this here to get some prices https://ec2instances.info

This might help too https://github.com/templed/ec2-instances-sql


# Can be good to store wheels in s3 or dbfs
# Using the pip install here as the wheel keeps changing during this development
# In the notebook the wheel can be re-installed via
%pip install /dbfs/project-env-py3-none-any.whl


def recurse_dbutils_fs_ls(s3dir):
    """
    Recursive ls of s3 directory using dbutils
    :param s3dir: The s3 prefix to iterate through
    :return: Iterator of all files with that prefix
    """
    import itertools
    results = dbutils.fs.ls(s3dir)
    if len(results) == 1 and results[0].path == s3dir:
        # Base case here
        yield results[0]
    else:
        result_paths = map(lambda result: result.path, results)
        for item in itertools.chain.from_iterable(map(recurse_dbutils_fs_ls, result_paths)):
            yield item

dbutils.fs.ls size is in bytes
To get the GB Size for a single file
size_gb = dbutils.fs.ls('s3://stuff.json')[0] / (1024*1024*1024)

# When creating a parquet table from data in s3
# use "ALTER TABLE schema.tablename RECOVER PARTITIONS" to determine the partitions
# otherwise partitioned tables will all return empty queries
# like

CREATE TABLE aschema.atable
USING PARQUET
LOCATION 's3://bucket/path/';
SELECT * FROM aschema.atable;  # will ALWAYS be empty if the directory is partitioned like 's3://bucket/path/stuff=hello/'
-- Then run
ALTER TABLE aschema.atable RECOVER PARTITIONS;
SELECT * FROM aschema.atable; -- will work!
# MSCK REPAIR TABLE aschema.atable; works too




@patch("data_processing.lib.aws_utils.s3fs")
def test_s3_list_dir_glob(mock_s3fs):
    """basic test"""
    mock_s3fs.S3FileSystem.return_value.glob.return_value = [
        "bucket12/stuff/p1/hello/world.parquet", "bucket12/stuff/p1/hello/earth.parquet",
        "bucket12/stuff/p2/hello/world.parquet", "bucket12/stuff/p2/hello/world.parquet"]
    result = s3_list_dir_glob("*/glob/*/", 'thebucket')
    exp_result = ["stuff/p1/hello/world.parquet", "stuff/p1/hello/earth.parquet",
                  "stuff/p2/hello/world.parquet", "stuff/p2/hello/world.parquet"]
    assert set(result) == set(exp_result)
    mock_s3fs.S3FileSystem.assert_called_once()
    mock_s3fs.S3FileSystem.return_value.glob.assert_called_once_with("s3://thebucket/*/glob/*/")

def s3_list_dir_glob(bucket: str, glob_path: str, ) -> List[str]:
    """
    Glob a s3 path and return all found paths
    :param bucket: S3 Bucket
    :param glob_path: An s3 prefix path suitable for globbing like stuff/*/otherstuff/*.parquet
       does not include the s3://bucket/
    :return: A list of prefixes (i.e. bucket is not in the output)
    """
    glob_result = s3fs.S3FileSystem().glob(f"s3://{PurePosixPath(bucket, glob_path)}")
    # Result is like "bucket/glob_path/" so wipe out the bucket/ to return just the paths found
    return [str(PurePosixPath(*PurePosixPath(apath).parts[1:])) for apath in glob_result]


def dbutils_s3_path_exists(a_path):
    try:
        dbutils.fs.ls(a_path)
        return True
    except Exception as e:
        if 'java.io.FileNotFoundException' in str(e):
            return False
        else:
            raise


# reference a delta path in a sql statement as delta.`s3://path/here/`
spark.sql(f"ANALYZE TABLE delta.`s3://path/here/` COMPUTE DELTA STATISTICS")