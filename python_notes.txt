
# Bit of code that can be used to store stack traces
with open('out.txt', 'a') as f:
    import traceback
    for line in traceback.format_stack():
        f.write(line)


class ArgParseFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):
    """
    Helper class for ArgParse to have both better formatting
    Usage: parser = argparse.ArgumentParser(description=description, formatter_class=ArgParseFormatter)

    Formatting of description and arguments will be wider and will automatically print any default argument values.
    """
    pass

# Pretty print a dictionary
import pprint
pp = pprint.PrettyPrinter(indent=4)
# Dictionary has to be long enough to get value from pprint otherwise it's printed on a single line
pp.pprint(dict(thisiskey1=18, key2hello='hello', worldisgood='z', otherstuff='fisoaf'))


Pycharm Multi line cursor
Shift + Option + Mouse drag

*** sklearn ***
roc_auc_score returns the AUC as per the documentation https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score


# 2 class confusion matrix
pd.DataFrame(confusion_matrix(y_test, predictions),
             index=pd.MultiIndex.from_tuples([('actual', False), ('actual', True)]),
             columns=pd.MultiIndex.from_tuples([('predicted', False), ('predicted', True)]))

# All ordered combinations of a list
lst = ['a' ,'b', 'c']
list(itertools.permutations(lst, 2))
# [('a', 'b'), ('a', 'c'), ('b', 'a'), ('b', 'c'), ('c', 'a'), ('c', 'b')]

# Unordered combinations of a list
lst = ['a' ,'b', 'c']
list(itertools.combinations(lst, 2))
# [('a', 'b'), ('a', 'c'), ('b', 'c')]

# arrow print date as 2d numbers
now = arrow.now()
# now = arrow.get('0010-2-3')
print(f"{now.year:04d}_{now.month:02d}_{now.day:02d}")

# Pip install from repo
pip install git+ssh://git@github.com/reponame.git@dev#subdirectory=subdir123
pip install git+ssh://[STUFF FROM GIT CLONE HERE]@[BRANCHNAME]#subdirectory=[SUBDIRECTORY WITH setup.py SCRIPT]


"""Print the size of all the files in an s3 directory"""
def print_s3_du_size(bucket, prefix):
  import boto3
  s3 = boto3.client('s3')

  # Create a reusable Paginator
  paginator = s3.get_paginator('list_objects_v2')

  # Create a PageIterator from the Paginator
  page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)

  the_list = [page for page in page_iterator]

  byte_size = sum([content['Size'] for apage in the_list for content in apage['Contents']])
  print(f"Temp size is: {byte_size / 1024**3} GBs")
  print(f"Temp size is: {byte_size / 1024**4} TBs")


"""Regex Filter a list"""





### Troubleshooting unittests running through PyCharm
Error looks like:
```
from ..test_util import Something
ValueError: attempted relative import beyond top-level package
```
This can be fixed by updating the Working Directory to the absolute directory of this repository such as
`/Users/stephenpettinato/Projects/the_project_dir_name/`.

Steps
1. Run the test, it fails
2. Run-Edit Configurations
3. Select failing test and change "Working Directory"

# Path to my python executable
import sys
sys.executable

# Pull the version of python from sys
import sys
sys.version


""" Read a file directly from s3 as bytes """
"""Example with parquet file"""
import pandas as pd
import boto3
from io import BytesIO
client = boto3.session.Session().client("s3")
obj = client.get_object(Bucket=bucket, Key=parquet_file)['Body'].read()
pd.read_parquet(BytesIO(obj))

"""Example with XGBoost model"""
import boto3
from io import BytesIO
import xgboost as xgb
client = boto3.session.Session().client("s3")
obj = client.get_object(Bucket=bucket, Key=model_file_in_s3)['Body'].read()
xgb_model = xgb.Booster()
xgb_model.load_model(bytearray(BytesIO(obj).read()))
print(type(xgb_model))
print(xgb_model.attributes())


# Iterate over a iterable in chunks
import itertools
x = list(range(100))

def grouper(iterable, n):
    """n is the size of the chunks you want"""
    args = [iter(iterable)] * n
    # zip_longest makes the last chunk of size n with None used as filler
    return itertools.zip_longest(*args)

for grp in grouper(x, 11):
    # Remove the None before processing
    clean_grp = [item for item in grp if item is not None]
    print(f"Group: {clean_grp}")

# Boto3 get the size of a single directory in s3
import boto3
bucket = "a_bucket"
s3_dir = "an_s3_dir"
bucket_resource = boto3.resource("s3").Bucket(bucket)
dir_size = sum(map(lambda item: item.size, bucket_resource.objects.filter(Prefix=s3_dir).all()))

# Helper to delete extra AWS Glue tables - assumes credentials are setup as standard
from boto3.session import Session
glue_client = Session().client("glue")
def del_tables_in_db(db_name, loop_count=10):
  for i in range(loop_count):
    print(f"{db_name}: Running loop {i} of {loop_count} to delete all tables in the database")
    # MaxResults > 100 doesn't work
    tbls = [tbl['Name'] for tbl in glue_client.get_tables(DatabaseName=db_name, MaxResults=100)['TableList']]
    if len(tbls) == 0:
        print(f'{db_name}: No Tables to Delete')
        break
    else:
        print(f'{db_name}: Deleting {len(tbls)} tables')
        glue_client.batch_delete_table(DatabaseName=db_name, TablesToDelete=tbls)

Format String
x = 88.5435432142
print(f"x formatted = {x:.2f}")

# Sum of a directory in s3
import s3fs
s3fs_fs = s3fs.S3FileSystem()

# Sum in GBs
sum([
    s3fs_fs.size(item) / (1024*1024*1024)
    for item in s3fs_fs.glob('s3://some_bucket/subdir/subdir2/**')
    if item.endswith(".parquet")
    and "_delta_log" not in item  # If it's a delta table
])


# Print out the version of an installed module that can't be imported
# i.e. when
# import sentence_transformers failed, I did the below to get the installed version

# 1. Get a path to the module's init file
import importlib.util
spec = importlib.util.find_spec("sentence_transformers")
init_path = spec.origin
print(init_path)

# 2. Print out the contents of the init file and hope it has the version number
with open(init_path, 'r') as afile:
    print(afile.read())


x = 88.5435432142
print(f"x formatted = {x:.2f}")
